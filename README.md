# Seeing through Satellite Images at Street Views (TPAMI 2026)

[![arXiv](https://img.shields.io/badge/arXiv-2505.17001-b31b1b.svg)](https://arxiv.org/abs/2505.17001)
[![Paper](https://img.shields.io/badge/IEEE_TPAMI-10.1109/TPAMI.2026.3652860-blue)](https://doi.org/10.1109/TPAMI.2026.3652860)
[![Project Page](https://img.shields.io/badge/Project-Page-green)](https://qianmingduowan.github.io/sat2density-pp/)

This is the official implementation of **Sat2Density++**.

---

### üìù TL;DR
We propose **Sat2Density++**, a novel framework for high-quality street-view video synthesis from satellite imagery:
- **Minimal Training Requirements**: It only requires $N$ pairs of GPS-matched satellite and street-view panorama images. **No** video data and **no** 3D annotations are needed during training.
- **Flexible Inference**: Given a single satellite image and a user-defined driving trajectory, the model generates a temporally consistent panorama video.
- **Superior Performance**: Sat2Density++ significantly outperforms the previous conference version (**Sat2Density**). Visual comparisons are available on our [Project Page](https://qianmingduowan.github.io/sat2density-pp/).

---

### üìÖ TODO List
- [ ] **By Jan 24**: Update the arXiv preprint to the journal version.
- [ ] **By Jan 31**: Release the inference code.
- [ ] **By Feb 16**: Release the training code.

---

### üì• Installation & Usage
*(The code will be released sequentially according to the timeline above. Stay tuned!)*

---

### üìú Citation
If you find our work or code useful for your research, please cite:

```bibtex
@ARTICLE{11344749,
  author={Qian, Ming and Tan, Bin and Wang, Qiuyu and Zheng, Xianwei and Xiong, Hanjiang and Xia, Gui-Song and Shen, Yujun and Xue, Nan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Seeing through Satellite Images at Street Views}, 
  year={2026},
  volume={},
  number={},
  pages={1-18},
  doi={10.1109/TPAMI.2026.3652860}
}
```

---

### üí¨ Contact & Discussion
We welcome any questions, discussions, or feedback regarding the paper or the implementation. Feel free to open an **issue** or reach out via email!
